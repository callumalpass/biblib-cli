---
id: 4B86443T
type: article
abstract: "Mechanistic interpretability (MI) aims to explain how neural networks work by uncovering their underlying causal mechanisms. As the field grows in influence, it is increasingly important to examine not just models themselves, but the assumptions, concepts and explanatory strategies implicit in MI research. We argue that mechanistic interpretability needs philosophy: not as an afterthought, but as an ongoing partner in clarifying its concepts, refining its methods, and assessing the epistemic and ethical stakes of interpreting AI systems. Taking three open problems from the MI literature as examples, this position paper illustrates the value philosophy can add to MI research, and outlines a path toward deeper interdisciplinary dialogue."
DOI: 10.48550/arXiv.2506.18852
note: arXiv:2506.18852 [cs]
number: arXiv:2506.18852
publisher: arXiv
source: arXiv.org
title: Mechanistic Interpretability Needs Philosophy
URL: http://arxiv.org/abs/2506.18852
author:
  - family: Williams
    given: Iwan
  - family: Oldenburg
    given: Ninell
  - family: Dhar
    given: Ruchira
  - family: Hatherley
    given: Joshua
  - family: Fierro
    given: Constanza
  - family: Rajcic
    given: Nina
  - family: Schiller
    given: Sandrine R.
  - family: Stamatiou
    given: Filippos
  - family: SÃ¸gaard
    given: Anders
accessed:
  date-parts:
    - - "2026"
      - 2
      - 10
issued:
  date-parts:
    - - "2025"
      - 6
      - 23
attachment: &a1
  - biblib/4B86443T/Mechanistic Interpretability Needs Philosophy.pdf
attachments: *a1
pdflink: biblib/4B86443T/Mechanistic Interpretability Needs Philosophy.pdf
tags:
  - literature_note
aliases:
  - Mechanistic Interpretability Needs Philosophy
authorLink: '["[[Author/]]",]'
related: '["",]'
---
